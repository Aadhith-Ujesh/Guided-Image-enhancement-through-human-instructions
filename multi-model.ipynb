{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = bert_model(**inputs)\n",
    "    # Obtain the embeddings by averaging the last hidden states\n",
    "    embeddings = outputs.last_hidden_state.mean(1)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class MultimodalCrossAttention(nn.Module):\n",
    "#     def __init__(self, text_embedding_dim, image_channels, hidden_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.text_projection = nn.Linear(text_embedding_dim, hidden_dim)\n",
    "#         self.image_projection = nn.Conv2d(image_channels, hidden_dim, kernel_size=1)  # Project to the same hidden_dim\n",
    "#         self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "        \n",
    "#     def forward(self, text_embeddings, image_latents):\n",
    "#         # Project text embeddings\n",
    "#         text_features = self.text_projection(text_embeddings)  # Shape: [batch_size, hidden_dim]\n",
    "#         text_features = text_features.permute(1, 0)  # Shape for attention: [seq_len, batch_size, hidden_dim]\n",
    "        \n",
    "#         # Project image features\n",
    "#         batch_size, _, _ = image_latents.shape\n",
    "#         image_features = self.image_projection(image_latents)  # Shape: [batch_size, hidden_dim, H, W]\n",
    "#         image_features = image_features.flatten(2)  # Shape: [batch_size, hidden_dim, H*W]\n",
    "#         image_features = image_features.permute(2, 0, 1)  # Shape for attention: [seq_len, batch_size, hidden_dim]\n",
    "        \n",
    "#         # Apply cross-attention\n",
    "#         # Here, we use image features as queries and text features as keys and values\n",
    "#         attn_output, _ = self.attention(query=image_features, key=text_features, value=text_features)\n",
    "        \n",
    "#         # Post-processing of attention output can be added here\n",
    "        \n",
    "#         return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "(1, 64, 64, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     latent_image \u001b[38;5;241m=\u001b[39m latent_image\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get combined features\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_features\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [27], line 17\u001b[0m, in \u001b[0;36mMultimodalCrossAttention.forward\u001b[1;34m(self, text_embeddings, image_latents)\u001b[0m\n\u001b[0;32m     14\u001b[0m text_features \u001b[38;5;241m=\u001b[39m text_features\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape for attention: [seq_len, batch_size, hidden_dim]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Project image features\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m batch_size, _, _ \u001b[38;5;241m=\u001b[39m image_latents\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     18\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_projection(image_latents)  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, H, W]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m image_features \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch_size, hidden_dim, H*W]\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# text = \"Example text for processing\"\n",
    "# text_embeddings = get_bert_embeddings(text)  # [1, 768] for single sentence\n",
    "# print(text_embeddings.shape)\n",
    "\n",
    "# # Simulate a latent image tensor of shape [1, 3, 64, 64]\n",
    "# latent_image = np.load(\"D:/FYP/latent_vector.npy\")  # Example latent image tensor\n",
    "# print(latent_image.shape)\n",
    "\n",
    "# # Instantiate the multimodal model\n",
    "# model = MultimodalCrossAttention(text_embedding_dim=768,  # Dimension of BERT-base embeddings\n",
    "#                                     image_channels=3, hidden_dim=512, num_heads=8)\n",
    "\n",
    "# # Ensure text embeddings and latent image are on the same device\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     text_embeddings = text_embeddings.cuda()\n",
    "#     latent_image = latent_image.cuda()\n",
    "\n",
    "# # Get combined features\n",
    "# combined_features = model(text_embeddings, latent_image)\n",
    "\n",
    "# print(combined_features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([1, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming your inputs\n",
    "text_embedding = get_bert_embeddings(\"hello im aadhith\")  # [Batch, TextEmbeddingDim]\n",
    "latent_image = np.load(\"D:/FYP/latent_vector.npy\")  # [Batch, H, W, Channels]\n",
    "latent_vector_tensor = torch.from_numpy(latent_image)\n",
    "\n",
    "# Convert image to PyTorch's channel-first format and apply adaptive avg pooling\n",
    "latent_vector_tensor = latent_vector_tensor.permute(0, 3, 1, 2)  # [Batch, Channels, H, W]\n",
    "pooled_image = F.adaptive_avg_pool2d(latent_vector_tensor, (8, 8))  # Reduce spatial dimensions\n",
    "image_features = pooled_image.flatten(start_dim=2)  # [Batch, Channels, NewH*NewW]\n",
    "image_features = image_features.permute(0, 2, 1)  # Prepare for attention [Batch, SeqLen, Channels]\n",
    "\n",
    "print(text_embedding.shape)\n",
    "print(image_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 768])\n",
      "torch.Size([1, 64, 768])\n",
      "torch.Size([1, 64, 64])\n",
      "tensor([[[-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425],\n",
      "         [-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425],\n",
      "         [-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425],\n",
      "         ...,\n",
      "         [-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425],\n",
      "         [-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425],\n",
      "         [-0.4287, -0.3994, -0.7315,  ...,  0.0175, -0.2980,  0.0425]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "query = text_embedding.unsqueeze(0)  # Add sequence length dimension\n",
    "query.shape\n",
    "key_value = image_features\n",
    "key_value.shape\n",
    "\n",
    "# Step 1: Define a linear projection layer for tensor2\n",
    "projection_layer = nn.Linear(in_features=3, out_features=768)\n",
    "\n",
    "# Step 2: Apply the linear projection to tensor2\n",
    "# Reshape tensor2 to [64, 3] to apply linear projection, then reshape back to [1, 64, 768]\n",
    "tensor2_projected = projection_layer(key_value.view(-1, 3)).view(1, 64, 768)\n",
    "\n",
    "# Step 3: Expand tensor1 to match the sequence length of tensor2_projected\n",
    "tensor1_expanded = query.expand(-1, 64, -1)  # Size: [1, 64, 768]\n",
    "\n",
    "print(tensor1_expanded.shape)\n",
    "print(tensor2_projected.shape)\n",
    "\n",
    "query = tensor1_expanded\n",
    "key_value = tensor2_projected\n",
    "attention_scores = torch.bmm(tensor1_expanded, tensor2_projected.transpose(1, 2))\n",
    "print(attention_scores.shape)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Compute the weighted sum of values\n",
    "attention_output = torch.bmm(attention_weights, tensor2_projected)\n",
    "\n",
    "print(attention_output)\n",
    "print(attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 768])\n",
      "torch.Size([1, 64, 768])\n",
      "[tensor(-3.5360, grad_fn=<AddBackward0>), tensor(-3.6139, grad_fn=<AddBackward0>), tensor(-3.5560, grad_fn=<AddBackward0>), tensor(-3.4886, grad_fn=<AddBackward0>), tensor(-3.4993, grad_fn=<AddBackward0>), tensor(-3.4754, grad_fn=<AddBackward0>), tensor(-3.4880, grad_fn=<AddBackward0>), tensor(-3.4744, grad_fn=<AddBackward0>), tensor(-3.5635, grad_fn=<AddBackward0>), tensor(-3.6198, grad_fn=<AddBackward0>), tensor(-3.5684, grad_fn=<AddBackward0>), tensor(-3.5324, grad_fn=<AddBackward0>), tensor(-3.5239, grad_fn=<AddBackward0>), tensor(-3.5065, grad_fn=<AddBackward0>), tensor(-3.5538, grad_fn=<AddBackward0>), tensor(-3.5725, grad_fn=<AddBackward0>), tensor(-3.4120, grad_fn=<AddBackward0>), tensor(-3.4404, grad_fn=<AddBackward0>), tensor(-3.3314, grad_fn=<AddBackward0>), tensor(-3.2938, grad_fn=<AddBackward0>), tensor(-3.2256, grad_fn=<AddBackward0>), tensor(-3.3041, grad_fn=<AddBackward0>), tensor(-3.5448, grad_fn=<AddBackward0>), tensor(-3.5797, grad_fn=<AddBackward0>), tensor(-2.7608, grad_fn=<AddBackward0>), tensor(-2.9749, grad_fn=<AddBackward0>), tensor(-3.0765, grad_fn=<AddBackward0>), tensor(-3.1058, grad_fn=<AddBackward0>), tensor(-3.0150, grad_fn=<AddBackward0>), tensor(-3.0601, grad_fn=<AddBackward0>), tensor(-3.3457, grad_fn=<AddBackward0>), tensor(-3.3856, grad_fn=<AddBackward0>), tensor(-2.6553, grad_fn=<AddBackward0>), tensor(-2.7174, grad_fn=<AddBackward0>), tensor(-2.8872, grad_fn=<AddBackward0>), tensor(-2.9106, grad_fn=<AddBackward0>), tensor(-2.9802, grad_fn=<AddBackward0>), tensor(-2.9830, grad_fn=<AddBackward0>), tensor(-3.0348, grad_fn=<AddBackward0>), tensor(-3.1041, grad_fn=<AddBackward0>), tensor(-2.7993, grad_fn=<AddBackward0>), tensor(-2.6690, grad_fn=<AddBackward0>), tensor(-2.7207, grad_fn=<AddBackward0>), tensor(-2.8211, grad_fn=<AddBackward0>), tensor(-2.7862, grad_fn=<AddBackward0>), tensor(-2.6967, grad_fn=<AddBackward0>), tensor(-2.7398, grad_fn=<AddBackward0>), tensor(-2.7401, grad_fn=<AddBackward0>), tensor(-2.7525, grad_fn=<AddBackward0>), tensor(-2.7819, grad_fn=<AddBackward0>), tensor(-2.7823, grad_fn=<AddBackward0>), tensor(-2.7894, grad_fn=<AddBackward0>), tensor(-2.8055, grad_fn=<AddBackward0>), tensor(-2.8204, grad_fn=<AddBackward0>), tensor(-2.7892, grad_fn=<AddBackward0>), tensor(-2.7144, grad_fn=<AddBackward0>), tensor(-2.7370, grad_fn=<AddBackward0>), tensor(-2.7031, grad_fn=<AddBackward0>), tensor(-2.7307, grad_fn=<AddBackward0>), tensor(-2.7236, grad_fn=<AddBackward0>), tensor(-2.7517, grad_fn=<AddBackward0>), tensor(-2.7289, grad_fn=<AddBackward0>), tensor(-2.7007, grad_fn=<AddBackward0>), tensor(-2.6958, grad_fn=<AddBackward0>)]\n",
      "torch.Size([64])\n",
      "768\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [87], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m dk \u001b[38;5;241m=\u001b[39m key_value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(dk)\n\u001b[1;32m---> 32\u001b[0m scaled_attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Apply softmax to get the attention weights\u001b[39;00m\n\u001b[0;32m     35\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scaled_attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "# # tensor_a = torch.randn(1, 64, 768)  # Q\n",
    "# # tensor_b = torch.randn(1, 64, 768)  # K and V\n",
    "# # Calculate the dot product between Q and K\n",
    "# # Compute the dot product along the last dimension\n",
    "\n",
    "    \n",
    "# tensor1 = query\n",
    "# tensor2 = key_value\n",
    "\n",
    "# print(tensor1.shape)\n",
    "# print(tensor2.shape)\n",
    "# result = []\n",
    "\n",
    "# # Iterate over the middle dimension\n",
    "# for i in range(64):\n",
    "#     dot_product = 0\n",
    "#     # Compute dot product for the i-th vector\n",
    "#     for j in range(768):\n",
    "#         try:\n",
    "#             dot_product += tensor1[0][i][j] * tensor2[0][i][j]\n",
    "#         except:\n",
    "#             print(i,j)\n",
    "#     result.append(dot_product)\n",
    "\n",
    "# print(result)\n",
    "# # attention_scores = torch.matmul(query, key_value.transpose(-2, -1))\n",
    "# attention_scores = torch.tensor(result)\n",
    "# print(attention_scores.size())\n",
    "# # Scale scores by the square root of the dimension of the keys\n",
    "# dk = key_value.size(-1)\n",
    "# print(dk)\n",
    "# scaled_attention_scores = attention_scores / torch.sqrt(dk)\n",
    "\n",
    "# # Apply softmax to get the attention weights\n",
    "# attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
    "\n",
    "# # Multiply by V\n",
    "# output = torch.matmul(attention_weights, key_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoderLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [94], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# the number of heads in the multiheadattention models\u001b[39;00m\n\u001b[0;32m     43\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# the dropout value\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Example input tensor\u001b[39;00m\n\u001b[0;32m     48\u001b[0m src \u001b[38;5;241m=\u001b[39m attention_output \u001b[38;5;66;03m# [sequence length, batch size, feature size]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [94], line 27\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, ntoken, ninp, nhead, nhid, nlayers, dropout)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m PositionalEncoding(ninp, dropout)\n\u001b[1;32m---> 27\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoderLayer\u001b[49m(ninp, nhead, nhid, dropout)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder \u001b[38;5;241m=\u001b[39m TransformerEncoder(encoder_layer\u001b[38;5;241m=\u001b[39mencoder_layers, num_layers\u001b[38;5;241m=\u001b[39mnlayers)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mninp \u001b[38;5;241m=\u001b[39m ninp\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TransformerEncoderLayer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, feature_size, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(feature_size, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=feature_size, nhead=nhead, dim_feedforward=nhid, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=nlayers)\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src * math.sqrt(self.feature_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "# Parameters for the Transformer model\n",
    "feature_size = 768  # Feature size (embedding dimension)\n",
    "nhead = 8  # Number of heads in the multiheadattention models\n",
    "nhid = 768  # Dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6  # Number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2  # Dropout value\n",
    "\n",
    "model = TransformerEncoderModel(feature_size, nhead, nhid, nlayers, dropout)\n",
    "\n",
    "# Assuming input tensor is of shape [batch size, sequence length, features]\n",
    "input_tensor = attention_output\n",
    "\n",
    "# Transpose the input to match the expected format [sequence length, batch size, features]\n",
    "input_tensor_transposed = input_tensor.transpose(0, 1)  # Shape: [64, 1, 768]\n",
    "\n",
    "output = model(input_tensor_transposed)\n",
    "print(output.shape)  # Should be [64, 1, 768] (sequence length, batch size, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 64, 3])\n",
      "Adjusted Key/Value Shape: torch.Size([1, 64, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 8, 96]' is invalid for input of size 49152",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjusted Key/Value Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, adjusted_key_value_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply cross-attention\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_key_value_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [54], line 8\u001b[0m, in \u001b[0;36mCrossAttentionModule.forward\u001b[1;34m(self, query, key_value)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key_value):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Cross-attention: Query from text, Key/Value from image\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\ujesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:5346\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5344\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   5345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 5346\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   5347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5348\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[0;32m   5350\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz \u001b[38;5;241m*\u001b[39m num_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 8, 96]' is invalid for input of size 49152"
     ]
    }
   ],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
    "        \n",
    "    def forward(self, query, key_value):\n",
    "        # Cross-attention: Query from text, Key/Value from image\n",
    "        attn_output, _ = self.attention(query=query, key=key_value, value=key_value)\n",
    "        return attn_output\n",
    "\n",
    "class AdjustKeyValueDimensions(nn.Module):\n",
    "    def __init__(self, key_value_dim, target_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(key_value_dim, target_embedding_dim)\n",
    "\n",
    "    def forward(self, key_value):\n",
    "        batch_size, seq_len, _ = key_value.shape\n",
    "        key_value_flat = key_value.view(batch_size * seq_len, -1)  # Flatten to [batch_size*seq_len, key_value_dim]\n",
    "        projected = self.projection(key_value_flat)  # Project to [batch_size*seq_len, target_embedding_dim]\n",
    "        projected = projected.view(batch_size, seq_len, -1)  # Reshape back to [batch_size, seq_len, target_embedding_dim]\n",
    "        return projected\n",
    "\n",
    "# Initialize module\n",
    "embed_size = 768  # Assuming text embedding size\n",
    "num_heads = 8  # Choose based on your model architecture\n",
    "cross_attention = CrossAttentionModule(embed_size, num_heads)\n",
    "\n",
    "# Prepare inputs\n",
    "# Text embedding as query: [SeqLen, Batch, EmbeddingDim]\n",
    "query = text_embedding.unsqueeze(0)  # Add sequence length dimension\n",
    "print(query.shape)\n",
    "# Image features as key/value: [SeqLen, Batch, EmbeddingDim]\n",
    "key_value = image_features\n",
    "print(image_features.shape)\n",
    "\n",
    "\n",
    "# Adjust the key/value tensor\n",
    "adjuster = AdjustKeyValueDimensions(key_value_dim=3, target_embedding_dim=768)\n",
    "adjusted_key_value_tensor = adjuster(key_value)\n",
    "\n",
    "# Now both tensors are compatible for cross-attention\n",
    "print(\"Adjusted Key/Value Shape:\", adjusted_key_value_tensor.shape)\n",
    "\n",
    "\n",
    "# Apply cross-attention\n",
    "attn_output = cross_attention(query, adjusted_key_value_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
